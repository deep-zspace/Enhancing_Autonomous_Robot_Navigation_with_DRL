{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import os\n",
    "import gym\n",
    "import torch\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import trange\n",
    "from ppo_torch.ppo_agent import PPOAgent\n",
    "from envs.escape_room_continuous_space_env import EscapeRoomEnv\n",
    "import seaborn as sns\n",
    "from constants import (\n",
    "    CHECKPOINT_RADIUS,\n",
    "    ENV_HEIGHT,\n",
    "    ENV_WIDTH,\n",
    "    MAX_WHEEL_VELOCITY,\n",
    "    SCALE_FACTOR,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def train_ppo_agent(env, n_episodes=5000, update_interval=500):\n",
    "    agent = PPOAgent(\n",
    "        state_dim=env.observation_space.shape[0],\n",
    "        action_dim=env.action_space.shape[0],\n",
    "        lr_actor=0.0003,\n",
    "        lr_critic=0.001,\n",
    "        gamma=0.99,\n",
    "        K_epochs=40,\n",
    "        eps_clip=0.2,\n",
    "        total_updates=n_episodes // update_interval,\n",
    "        action_std_init=0.6\n",
    "    )\n",
    "\n",
    "    plot_dir = 'plots'\n",
    "    os.makedirs(plot_dir, exist_ok=True)\n",
    "    filename = f\"PPO_EscapeRoom_{n_episodes}_episodes\"\n",
    "    figure_file = f\"{plot_dir}/{filename}.png\"\n",
    "\n",
    "    score_history = []\n",
    "    loss_history = []\n",
    "\n",
    "    pbar = trange(n_episodes, desc='Initializing training...')\n",
    "\n",
    "    for i in pbar:\n",
    "        state, info = env.reset()\n",
    "        done = False\n",
    "        score = 0\n",
    "        steps = 0\n",
    "        local_losses = []\n",
    "\n",
    "        while not done:\n",
    "            reward = 0 \n",
    "            done = False\n",
    "            action, action_logprob, state_val = agent.select_action(state)\n",
    "            next_state, reward, terminated, truncated, info = env.step(action)\n",
    "            done = terminated or truncated\n",
    "            # Store full experience now that all values are known\n",
    "            agent.buffer.store(state, action, action_logprob, state_val, reward, done)\n",
    "            \n",
    "            score += reward\n",
    "            state = next_state\n",
    "            steps += 1\n",
    "\n",
    "            if steps % update_interval == 0 or done:\n",
    "                loss = agent.update()\n",
    "                local_losses.append(loss)\n",
    "\n",
    "\n",
    "        score_history.append(score)\n",
    "        avg_loss = np.mean(local_losses) if local_losses else 0\n",
    "        loss_history.append(avg_loss)\n",
    "\n",
    "        description = f\"Episode {i+1}: Score {score:.1f}, Info : {info['reason']} , Avg Score {np.mean(score_history[-100:]):.3f}, Avg Loss {avg_loss:.4f}\"\n",
    "        pbar.set_description(description)\n",
    "\n",
    "        if (i + 1) % (n_episodes // 10) == 0 or i == n_episodes - 1:\n",
    "            agent.save(os.path.join(agent.checkpoint_dir, f'checkpoint_{i + 1}.pth'))\n",
    "            plt.figure(figsize=(10, 5))\n",
    "            sns.lineplot(data=score_history)\n",
    "            plt.title('Score per Episode')\n",
    "            plt.xlabel('Episode')\n",
    "            plt.ylabel('Score')\n",
    "            plt.savefig(figure_file)\n",
    "            plt.close()\n",
    "\n",
    "    return {\n",
    "        \"no_of_episodes\": n_episodes,\n",
    "        \"score_history\": score_history,\n",
    "        \"loss_history\": loss_history,\n",
    "        \"figure_file\": figure_file\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/deep/.local/lib/python3.10/site-packages/gym/spaces/box.py:127: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "  logger.warn(f\"Box bound precision lowered by casting to {self.dtype}\")\n",
      "Initializing training...:   0%|          | 0/50 [00:00<?, ?it/s]/home/deep/data/NEU/RL/Project/Project_Deep/Enhancing_Autonomous_Robot_Navigation_with_DRL/ppo_torch/buffer.py:14: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  state_tensor = torch.tensor(state).clone().detach().to(self.device)\n",
      "/home/deep/data/NEU/RL/Project/Project_Deep/Enhancing_Autonomous_Robot_Navigation_with_DRL/ppo_torch/buffer.py:15: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  action_tensor = torch.tensor(action).clone().detach().to(self.device)\n",
      "/home/deep/data/NEU/RL/Project/Project_Deep/Enhancing_Autonomous_Robot_Navigation_with_DRL/ppo_torch/buffer.py:16: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  logprob_tensor = torch.tensor(logprob).clone().detach().to(self.device)\n",
      "/home/deep/data/NEU/RL/Project/Project_Deep/Enhancing_Autonomous_Robot_Navigation_with_DRL/ppo_torch/buffer.py:17: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  state_value_tensor = torch.tensor(state_value).clone().detach().to(self.device)\n",
      "Initializing training...:   0%|          | 0/50 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Could not infer dtype of NoneType",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[24], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m env \u001b[38;5;241m=\u001b[39m EscapeRoomEnv(max_steps_per_episode\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m3000\u001b[39m, goal\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m300\u001b[39m, \u001b[38;5;241m450\u001b[39m), delta\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m15\u001b[39m)\n\u001b[0;32m----> 4\u001b[0m training_data \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_ppo_agent\u001b[49m\u001b[43m(\u001b[49m\u001b[43menv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_episodes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m50\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mupdate_interval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[23], line 34\u001b[0m, in \u001b[0;36mtrain_ppo_agent\u001b[0;34m(env, n_episodes, update_interval)\u001b[0m\n\u001b[1;32m     32\u001b[0m reward \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m \n\u001b[1;32m     33\u001b[0m done \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m---> 34\u001b[0m action, action_logprob, state_val \u001b[38;5;241m=\u001b[39m \u001b[43magent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mselect_action\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstate\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     35\u001b[0m next_state, reward, terminated, truncated, info \u001b[38;5;241m=\u001b[39m env\u001b[38;5;241m.\u001b[39mstep(action)\n\u001b[1;32m     36\u001b[0m done \u001b[38;5;241m=\u001b[39m terminated \u001b[38;5;129;01mor\u001b[39;00m truncated\n",
      "File \u001b[0;32m~/data/NEU/RL/Project/Project_Deep/Enhancing_Autonomous_Robot_Navigation_with_DRL/ppo_torch/ppo_agent.py:53\u001b[0m, in \u001b[0;36mPPOAgent.select_action\u001b[0;34m(self, state)\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[38;5;66;03m# Ensure that act returns action, action_logprob, and state_val\u001b[39;00m\n\u001b[1;32m     51\u001b[0m action, action_logprob, state_val \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpolicy_old\u001b[38;5;241m.\u001b[39mact(state)\n\u001b[0;32m---> 53\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbuffer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstore\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maction\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maction_logprob\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstate_val\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Temporarily store None for reward and done\u001b[39;00m\n\u001b[1;32m     54\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m action\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mnumpy()\u001b[38;5;241m.\u001b[39mflatten(), action_logprob, state_val\n",
      "File \u001b[0;32m~/data/NEU/RL/Project/Project_Deep/Enhancing_Autonomous_Robot_Navigation_with_DRL/ppo_torch/buffer.py:18\u001b[0m, in \u001b[0;36mRolloutBuffer.store\u001b[0;34m(self, state, action, logprob, state_value, reward, done)\u001b[0m\n\u001b[1;32m     16\u001b[0m logprob_tensor \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor(logprob)\u001b[38;5;241m.\u001b[39mclone()\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[1;32m     17\u001b[0m state_value_tensor \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor(state_value)\u001b[38;5;241m.\u001b[39mclone()\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[0;32m---> 18\u001b[0m reward_tensor \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreward\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mclone()\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[1;32m     19\u001b[0m done_tensor \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor(done)\u001b[38;5;241m.\u001b[39mclone()\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[1;32m     21\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstates\u001b[38;5;241m.\u001b[39mappend(state_tensor)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Could not infer dtype of NoneType"
     ]
    }
   ],
   "source": [
    "env = EscapeRoomEnv(max_steps_per_episode=3000, goal=(300, 450), delta=15)\n",
    "\n",
    "\n",
    "training_data = train_ppo_agent(env, n_episodes=50, update_interval=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
