{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import os\n",
    "import gym\n",
    "import numpy as np\n",
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3.common.evaluation import evaluate_policy\n",
    "from tqdm import tqdm\n",
    "from stable_baselines3.common.vec_env import DummyVecEnv\n",
    "from envs.escape_room_continuous_space_env import EscapeRoomEnv\n",
    "from stable_baselines3.common.callbacks import BaseCallback, EvalCallback, CallbackList\n",
    "from stable_baselines3.common.monitor import Monitor\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomEnvWrapper(gym.Wrapper):\n",
    "    def __init__(self, env):\n",
    "        super().__init__(env)\n",
    "\n",
    "    def reset(self, **kwargs):\n",
    "        observation, info = self.env.reset(**kwargs)\n",
    "        return self._process_observation(observation)\n",
    "\n",
    "    def step(self, action):\n",
    "        observation, reward, terminated, truncated, info = self.env.step(action)\n",
    "        done = terminated or truncated\n",
    "        return self._process_observation(observation), reward, done, info\n",
    "\n",
    "    def _process_observation(self, observation):\n",
    "        if isinstance(observation, tuple):\n",
    "            return np.concatenate(observation)\n",
    "        else:\n",
    "            return observation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Custom Callback for collecting data\n",
    "class DataCollectorCallback(BaseCallback):\n",
    "    def __init__(self, verbose=0):\n",
    "        super(DataCollectorCallback, self).__init__(verbose)\n",
    "        self.rewards = []\n",
    "        self.losses = []\n",
    "        self.episode_lengths = []\n",
    "\n",
    "    def _on_step(self):\n",
    "        self.rewards.append(np.mean(self.training_env.get_attr('episode_rewards')))\n",
    "        self.episode_lengths.append(np.mean(self.training_env.get_attr('episode_lengths')))\n",
    "        if 'loss' in self.logger.Logger.CURRENT.output_formats[0].writer.data:\n",
    "            self.losses.append(self.logger.Logger.CURRENT.output_formats[0].writer.data['loss'])\n",
    "        return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/deep/.local/lib/python3.10/site-packages/gym/spaces/box.py:127: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "  logger.warn(f\"Box bound precision lowered by casting to {self.dtype}\")\n",
      "/home/deep/.local/lib/python3.10/site-packages/stable_baselines3/common/vec_env/patch_gym.py:49: UserWarning: You provided an OpenAI Gym environment. We strongly recommend transitioning to Gymnasium environments. Stable-Baselines3 is automatically wrapping your environments in a compatibility layer, which could potentially cause issues.\n",
      "  warnings.warn(\n",
      "                                                             "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 0, Mean reward: -13219.462023, Std reward: 0.0\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 10, Mean reward: -34.759566, Std reward: 0.0\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 20, Mean reward: -159.16718399999996, Std reward: 2.842170943040401e-14\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 30, Mean reward: 136.44583400000002, Std reward: 2.842170943040401e-14\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 40, Mean reward: 103.189574, Std reward: 0.0\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 50, Mean reward: 37.06732, Std reward: 0.0\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 60, Mean reward: 46.71934900000001, Std reward: 7.105427357601002e-15\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Goal 'G' reached in 483 steps with cumulative reward 11976.570104072105 for this episode.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 80, Mean reward: 45.010089, Std reward: 0.0\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "# Initialize the environment and wrap it\n",
    "env = CustomEnvWrapper(EscapeRoomEnv(max_steps_per_episode=3000, goal=(550, 450), delta=15))\n",
    "\n",
    "# Initialize the PPO model\n",
    "model = PPO(\"MlpPolicy\", env, verbose=0)\n",
    "\n",
    "# Set up tqdm progress bar\n",
    "total_episodes = 500\n",
    "progress_bar = tqdm(total=total_episodes, desc=\"Training Progress\", leave=False)\n",
    "\n",
    "# Path for saving the model\n",
    "model_path = \"./tmp/ppo/ppo_escape_room_checkpoint.zip\"\n",
    "\n",
    "# Train the model\n",
    "for episode in range(total_episodes):\n",
    "    # Perform a training step\n",
    "    model.learn(total_timesteps=1000)  # Adjust total_timesteps as needed\n",
    "\n",
    "    # Optionally evaluate the policy every 10 episodes\n",
    "    if episode % 10 == 0:\n",
    "        mean_reward, std_reward = evaluate_policy(model, model.get_env(), n_eval_episodes=10)\n",
    "        print(f\"Episode: {episode}, Mean reward: {mean_reward}, Std reward: {std_reward}\", end='\\r')\n",
    "\n",
    "    # Save the model every 10% of the episodes, overwrite the same file\n",
    "    if (episode + 1) % (total_episodes // 10) == 0:\n",
    "        model.save(model_path)\n",
    "\n",
    "    # Update progress bar\n",
    "    progress_bar.update(1)\n",
    "\n",
    "# Save the final model\n",
    "model.save(\"./tmp/ppo/ppo_escape_room_final.zip\")\n",
    "\n",
    "\n",
    "# Close the environment\n",
    "env.close()\n",
    "\n",
    "print(\"Training completed and model saved.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the environment and wrap it\n",
    "env = CustomEnvWrapper(EscapeRoomEnv(max_steps_per_episode=3000, goal=(550, 450), delta=15))\n",
    "\n",
    "# Load the trained model\n",
    "model_path = \"./tmp/ppo/ppo_escape_room_final.zip\"\n",
    "model = PPO.load(model_path, env=env)\n",
    "\n",
    "# Evaluate the model\n",
    "mean_reward, std_reward = evaluate_policy(model, env, n_eval_episodes=20, deterministic=True)\n",
    "\n",
    "print(f\"Evaluated model on {20} episodes: Mean reward = {mean_reward}, Std reward = {std_reward}\")\n",
    "\n",
    "# Optionally: Visualize the agent's performance\n",
    "try:\n",
    "    for _ in range(5):  # Run 5 episodes\n",
    "        obs = env.reset()\n",
    "        done = False\n",
    "        while not done:\n",
    "            action, _states = model.predict(obs, deterministic=True)\n",
    "            obs, reward, done, info = env.step(action)\n",
    "            env.render()  # Render the environment to visualize the agent's behavior\n",
    "finally:\n",
    "    env.close()  # Ensure the environment is closed properly\n",
    "\n",
    "print(\"Evaluation and visualization completed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def plot_rewards(rewards, rolling_window=10):\n",
    "    \"\"\" Plots the reward trend along with a rolling average.\n",
    "\n",
    "    Args:\n",
    "        rewards (list): List of rewards obtained per episode.\n",
    "        rolling_window (int): Window size for the rolling average.\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.plot(rewards, label='Reward per Episode')\n",
    "    plt.plot(np.convolve(rewards, np.ones(rolling_window)/rolling_window, mode='valid'), \n",
    "             label=f'{rolling_window}-Episode Rolling Average')\n",
    "    plt.title('Rewards Trend')\n",
    "    plt.xlabel('Episodes')\n",
    "    plt.ylabel('Reward')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_losses(losses):\n",
    "    \"\"\" Plots the training losses.\n",
    "\n",
    "    Args:\n",
    "        losses (list): List of loss values recorded during training.\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.plot(losses, label='Training Loss per Step')\n",
    "    plt.title('Loss Trend')\n",
    "    plt.xlabel('Training Steps')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_episode_lengths(episode_lengths):\n",
    "    \"\"\" Plots the length of each episode over time.\n",
    "\n",
    "    Args:\n",
    "        episode_lengths (list): List of episode lengths.\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.plot(episode_lengths, label='Episode Length')\n",
    "    plt.title('Episode Length Trend')\n",
    "    plt.xlabel('Episodes')\n",
    "    plt.ylabel('Length')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
