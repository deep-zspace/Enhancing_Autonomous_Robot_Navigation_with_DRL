{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import os\n",
    "import gym\n",
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "from ddpg_torch.ddpg_agent import Agent\n",
    "\n",
    "from envs.escape_room_continuous_space_env import EscapeRoomEnv\n",
    "from tqdm import trange"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def train_diff_robot_custom_env(alpha=0.0001, beta=0.001, tau=0.001, n_games=1000):\n",
    "    env = EscapeRoomEnv()\n",
    "    agent = Agent(\n",
    "        alpha=alpha,\n",
    "        beta=beta,\n",
    "        input_dims=env.observation_space.shape,\n",
    "        tau=tau,\n",
    "        batch_size=64,\n",
    "        fc1_dims=400,\n",
    "        fc2_dims=300,\n",
    "        n_actions=env.action_space.shape[0],\n",
    "    )\n",
    "\n",
    "    filename = f\"EscapeRoom_alpha_{agent.alpha}_beta_{agent.beta}_{n_games}_games\"\n",
    "    figure_file = f\"plots/{filename}.png\"\n",
    "    score_history = []\n",
    "    critic_losses = []\n",
    "    actor_losses = []\n",
    "\n",
    "    save_interval = n_games // 10  # Save model and plot every 10% of n_games\n",
    "    pbar = trange(n_games)\n",
    "\n",
    "    for i in pbar:\n",
    "        state, info = env.reset()\n",
    "        done = False\n",
    "        score = 0\n",
    "\n",
    "        while not done:\n",
    "            action = agent.choose_action(state)\n",
    "            next_state, reward, terminated, truncated, info = env.step(action)\n",
    "            done = terminated or truncated\n",
    "            agent.remember(state, action, reward, next_state, done)\n",
    "            learn_outputs = agent.learn()\n",
    "            if learn_outputs:\n",
    "                critic_loss, actor_loss = learn_outputs\n",
    "            else:\n",
    "                critic_loss, actor_loss = (\n",
    "                    0,\n",
    "                    0,\n",
    "                )  # Default values when learning doesn't happen\n",
    "\n",
    "            score += reward\n",
    "            state = next_state\n",
    "        \n",
    "        \n",
    "\n",
    "        score_history.append(score)\n",
    "        critic_losses.append(critic_loss)\n",
    "        actor_losses.append(actor_loss)\n",
    "        \n",
    "        avg_score = np.mean(\n",
    "            score_history\n",
    "        )  # Calculate average score after appending current score\n",
    "\n",
    "        if (\n",
    "            i % save_interval == 0 or i == n_games - 1\n",
    "        ):  # Save model and plot at intervals\n",
    "            agent.save_models()\n",
    "\n",
    "        pbar.set_description(\n",
    "            f\"Episode {i}: Score {score:.1f}, Info : {info['reason']}, Average Score {avg_score:.3f}, Actor Losses {actor_losses[-1]:.3f}, Critic Losses {critic_losses[-1]:.3f}\"\n",
    "        )\n",
    "\n",
    "    return score_history, critic_losses, actor_losses, figure_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/deep/.local/lib/python3.10/site-packages/gym/spaces/box.py:127: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "  logger.warn(f\"Box bound precision lowered by casting to {self.dtype}\")\n",
      "  0%|          | 0/1000 [00:00<?, ?it/s]/home/deep/data/NEU/RL/Project/new_dqn_project/final_update/ddpg_torch/ddpg_agent.py:223: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:275.)\n",
      "  state = T.tensor([observation], dtype=T.float).to(self.actor.device)\n",
      "Episode 53: Score 4080.3, Info : Goal_reached, Average Score -353.742, Actor Losses 2.588, Critic Losses 5.618:   5%|▌         | 54/1000 [01:05<30:06,  1.91s/it]      "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Goal 'G' reached in 1085 steps with cumulative reward 5523.249428372609 for this episode.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Episode 96: Score 6496.9, Info : Goal_reached, Average Score -718.943, Actor Losses -57.780, Critic Losses 28.280:  10%|▉         | 97/1000 [03:36<44:34,  2.96s/it]         "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Goal 'G' reached in 173 steps with cumulative reward 6266.664341183488 for this episode.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Episode 98: Score 5764.5, Info : Goal_reached, Average Score -683.163, Actor Losses -85.068, Critic Losses 203.605:  10%|▉         | 99/1000 [03:44<49:47,  3.32s/it]       "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Goal 'G' reached in 440 steps with cumulative reward 5856.54732121648 for this episode.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Episode 107: Score 6324.3, Info : Goal_reached, Average Score -769.969, Actor Losses -140.388, Critic Losses 54.496:  11%|█         | 108/1000 [04:24<36:55,  2.48s/it]        "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Goal 'G' reached in 227 steps with cumulative reward 6143.134454265237 for this episode.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Episode 108: Score 5327.6, Info : Goal_reached, Average Score -714.028, Actor Losses -109.288, Critic Losses 80.587:  11%|█         | 109/1000 [04:26<34:56,  2.35s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Goal 'G' reached in 663 steps with cumulative reward 5696.630650404789 for this episode.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Episode 109: Score 4326.6, Info : Goal_reached, Average Score -668.203, Actor Losses -117.292, Critic Losses 25.907:  11%|█         | 110/1000 [04:29<38:11,  2.58s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Goal 'G' reached in 1011 steps with cumulative reward 5546.486550187122 for this episode.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Episode 131: Score 6392.2, Info : Goal_reached, Average Score -906.420, Actor Losses -191.287, Critic Losses 14.130:  13%|█▎        | 132/1000 [06:17<55:49,  3.86s/it]         "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Goal 'G' reached in 224 steps with cumulative reward 6147.590853588412 for this episode.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Episode 144: Score 3589.6, Info : Goal_reached, Average Score -994.267, Actor Losses -199.750, Critic Losses 25.560:  14%|█▍        | 145/1000 [07:23<1:10:21,  4.94s/it]        "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Goal 'G' reached in 1448 steps with cumulative reward 5434.688891756947 for this episode.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Episode 153: Score 3541.4, Info : Goal_reached, Average Score -977.520, Actor Losses -211.044, Critic Losses 41.714:  15%|█▌        | 154/1000 [07:59<1:01:14,  4.34s/it]        "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Goal 'G' reached in 1395 steps with cumulative reward 5444.9037380709 for this episode.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Episode 155: Score 6262.1, Info : Goal_reached, Average Score -935.732, Actor Losses -207.799, Critic Losses 110.013:  16%|█▌        | 156/1000 [08:04<47:41,  3.39s/it]   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Goal 'G' reached in 265 steps with cumulative reward 6073.988383059912 for this episode.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Episode 162: Score -1880.4, Info : out_of_bounds, Average Score -988.260, Actor Losses -209.332, Critic Losses 26.686:  16%|█▋        | 163/1000 [08:32<58:24,  4.19s/it]     "
     ]
    }
   ],
   "source": [
    "score_history, critic_losses, actor_losses, figure_file = train_diff_robot_custom_env()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_training_results(x, scores, critic_losses, actor_losses, figure_file):\n",
    "    fig, axs = plt.subplots(3, 1, figsize=(10, 15))\n",
    "\n",
    "    # Plotting the scores\n",
    "    axs[0].plot(x, scores, label='Score per Episode', color='blue')\n",
    "    axs[0].set_title('Scores Over Episodes')\n",
    "    axs[0].set_xlabel('Episode')\n",
    "    axs[0].set_ylabel('Score')\n",
    "    axs[0].grid(True)\n",
    "    axs[0].legend()\n",
    "\n",
    "    # Plotting the critic losses\n",
    "    axs[1].plot(x, critic_losses, label='Critic Loss per Episode', color='red')\n",
    "    axs[1].set_title('Critic Loss Over Episodes')\n",
    "    axs[1].set_xlabel('Episode')\n",
    "    axs[1].set_ylabel('Loss')\n",
    "    axs[1].grid(True)\n",
    "    axs[1].legend()\n",
    "\n",
    "    # Plotting the actor losses\n",
    "    axs[2].plot(x, actor_losses, label='Actor Loss per Episode', color='green')\n",
    "    axs[2].set_title('Actor Loss Over Episodes')\n",
    "    axs[2].set_xlabel('Episode')\n",
    "    axs[2].set_ylabel('Loss')\n",
    "    axs[2].grid(True)\n",
    "    axs[2].legend()\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(figure_file)\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "episodes = list(range(1, len(score_history) + 1))\n",
    "plot_training_results(episodes, score_history, critic_losses, actor_losses, figure_file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_and_simulate(env, agent, n_episodes=5, max_steps=500):\n",
    "    rewards = []\n",
    "    steps_per_episode = []\n",
    "\n",
    "    for episode in range(n_episodes):\n",
    "        state, info = env.reset()\n",
    "        done = False\n",
    "        total_reward = 0\n",
    "        steps = 0\n",
    "\n",
    "        while not done:\n",
    "            env.render()  # Optional: Comment this out if you don't need to visually inspect the simulation\n",
    "            action = agent.choose_action(state)\n",
    "            state, reward, terminated, truncated, info = env.step(action)\n",
    "            done = terminated or truncated\n",
    "            total_reward += reward\n",
    "            steps += 1\n",
    "\n",
    "            if steps >= max_steps:\n",
    "                break\n",
    "\n",
    "        print(f\"Episode {episode + 1}: Total reward = {total_reward}, Steps = {steps}\")\n",
    "        rewards.append(total_reward)\n",
    "        steps_per_episode.append(steps)\n",
    "\n",
    "    env.close()  # Close the environment when done\n",
    "    return rewards, steps_per_episode\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    env = EscapeRoomEnv(max_steps_per_episode=500)\n",
    "    agent = Agent(\n",
    "        alpha=0.0001,\n",
    "        beta=0.001,\n",
    "        input_dims=env.observation_space.shape,\n",
    "        tau=0.001,\n",
    "        fc1_dims=400,\n",
    "        fc2_dims=300,\n",
    "        n_actions=env.action_space.shape[0],\n",
    "        batch_size=64,\n",
    "    )\n",
    "\n",
    "    # Assume agent.load_models is properly implemented\n",
    "    agent.load_models()\n",
    "\n",
    "    rewards, steps_per_episode = load_and_simulate(env, agent, n_episodes=5, max_steps=1000)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
